### SGD
$$
x_{t+1} = x_t - \alpha \nabla f(x_t)
$$
``` Python
for t in range(num_step):
	dw = compute_gradient(w)
	w -= learning_rate * dw
```

### SGD + Momentum
物理学的动量
Combine gradient at current point with velocity to get step used to update weights
- Build up “velocity” as a running mean of gradients 
- Rho gives “friction”; typically rho=0.9 or 0.99
$$
\begin{array}
vv_{t+1} = \rho v_t + \nabla f(x_t) \\
x_{t+1} = x_t - \alpha v_{t+1}
\end{array}
$$
```Python
v = 0
for t in range(num_step):
	dw = compute_gradient(w)
	v = rho * v + dw
	w -= learning_rate * v
```

or like
$$
\begin{array}
vv_{t+1} = \rho v_t - \alpha \nabla f(x_t) \\
x_{t+1} = x_t + v_{t+1}
\end{array}
$$
```Python
v = 0
for t in range(num_step):
	dw = compute_gradient(w)
	v = rho * v - learning_rate * dw
	w += v
```

### Nesterov Momentum
“Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction

$$
\begin{array}
	vv_{t+1} = \rho v_t - \alpha \nabla f(x_t+\rho v_t) \\
	x_{t+1} = x_t + v_{t+1}
\end{array}
$$
Change of variables $\tilde{x_t} = x_t + \rho v_t$, and rearrange:
$$
\begin{aligned}
	v_{t+1} &= \rho v_t - \alpha \nabla f(\tilde{x}_t) \\
	\tilde{x}_{t+1} &= \tilde{x}_t - \rho v_t + (1 + \rho)v_{t+1}\\
	 &= \tilde{x}_t + v_{t+1} + \rho (v_{t+1}-v_t)
\end{aligned}
$$

```Python
v = 0
for t in range(num_steps):
	dw = compute_gradient(w)
	old_v = v
	v = rho * v - learning_rate * dw
	w -= rho * old_v - (1 + rho) * v
```

### AdaGrad
Added element-wise scaling of the gradient based on the historical sum of squares in each dimension
```Python
grad_squared = 0
epsilon = 1e-7
for t in range(num_steps):
	dw = compute_gradient(w)
	grad_squared += dw * dw
	w -= learning_rate * dw / (grad_squared.sqrt() + epsilon)
```

>如何理解AdaGrad中，常见特征的学习率降低，而稀有特征的学习率较高？
>1.**常见特征**：
    - 对于常见特征，它们在训练数据中可能出现频率较高，因此其对应的梯度 gt,ig_{t, i}gt,i​ 一般会比较大。
    - 大的梯度意味着 gt,i2g_{t, i}^2gt,i2​ 较大，因此累积梯度平方和 Gt,iG_{t, i}Gt,i​ 也会相对较大。
    - 根据学习率的更新公式，θi\theta_iθi​ 对应的学习率会相对较小，因为 Gt,i+ϵ\sqrt{G_{t, i} + \epsilon}Gt,i​+ϵ​ 较大。
   2.**稀有特征**：
    - 对于稀有特征，它们在训练数据中出现频率较低，其对应的梯度 gt,ig_{t, i}gt,i​ 一般会比较小。
    - 小的梯度意味着 gt,i2g_{t, i}^2gt,i2​ 较小，因此累积梯度平方和 Gt,iG_{t, i}Gt,i​ 也会相对较小。
    - 根据学习率的更新公式，θi\theta_iθi​ 对应的学习率会相对较大，因为 Gt,i+ϵ\sqrt{G_{t, i} + \epsilon}Gt,i​+ϵ​ 较小。

### RMSProp(Root Mean Square Propagation):"Leaky Adagrad"
```Python
grad_squared = 0
epsilon = 1e-7
for t in range(num_steps):
	dw = compute_gradient(w)
	grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw
	w -= learning_rate * dw / (grad_squared.sqrt() + epsilon)
```

### Adam(almost): RMSProp + Momentum
```Python
moment1 = 0
moment2 = 0
epsilon = 1e-7
for t in range(1, num_steps + 1):
	dw = compute_gradient(w)
	moment1 = beta1 * moment1 + (1 - beta1) * dw
	moment2 = beta2 * moment2 + (1 - beta2) * dw * dw
	w -= learning_rate * moment1 / (moment2.sqrt() + epsilon)
```
在t=1时，moment1与moment2都很小，梯度估计偏向0，因此需要做Bias correction

```Python
moment1 = 0
moment2 = 0
epsilon = 1e-7
for t in range(1, num_steps + 1):
	dw = compute_gradient(w)
	moment1 = beta1 * moment1 + (1 - beta1) * dw
	moment2 = beta2 * moment2 + (1 - beta2) * dw * dw
	moment1_unbias = moment1 / (1 - beta1 ** t)
	moment2_unbias = moment2 / (1 - beta2 ** t)
	w -= learning_rate * moment1_unbias / (moment2_unbias.sqrt() + epsilon)
```

>Adam with beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3, 5e-4, 1e-4 is a great starting point for many models!

### 优化算法比较

![](../../img/Pasted%20image%2020240617194802.png)