![](../../img/Pasted%20image%2020240618143125.png)
Why change dot product to scaled dot product?
	- Large similarities will cause softmax to saturate and give vanishing gradients.
	- Variance:![](../../img/Pasted%20image%2020240618143733.png)

## Attention Layer
![](../../img/Pasted%20image%2020240618143852.png)

## Self-Attention Layer
![](../../img/Pasted%20image%2020240618144000.png)
**Permutation Equivarlant**

### Positional Encoding
### Masked Self-Attention Layer
![](../../img/Pasted%20image%2020240618144152.png)

### Multihead Self-Attention
![](../../img/Pasted%20image%2020240618144245.png)
![](../../img/Pasted%20image%2020240618144257.png)


![](../../img/Pasted%20image%2020240618144536.png)
## Transformer
- **Highly scalable, highly parallelizable**
- 