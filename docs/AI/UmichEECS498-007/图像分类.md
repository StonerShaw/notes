## 数据集
### MNIST
手写数字，28 ×28的灰度图像
“计算机视觉的果蝇”
![](../../img/Pasted%20image%2020240617161141.png)
### CIFAR10与CIFAR100
10/100 classes，32×32的RGB图像
![](../../img/Pasted%20image%2020240617161351.png)
### ImageNet
1000 classes，使用Top 5 accuracy作为metric
![](../../img/Pasted%20image%2020240617161453.png)
### MIT Places
![](../../img/Pasted%20image%2020240617161532.png)
## KNN
训练$O(1)$,推理$O(n)$，$n$为训练集大小
**超参数**：关于算法的choices，不能从训练数据中获得，在训练开始时就需要确定
![](../../img/Pasted%20image%2020240617162721.png)
## 维度灾难
为了在空间中实现均匀覆盖，所需的训练点数随着维度的增加呈指数增长。

## 线性分类器(单层感知机)
学习类别的超平面。无法学习非线性的特征

## 损失函数
损失函数可以评判分类器的好坏，帮助选择合适的参数
### 交叉熵损失
![](../../img/Pasted%20image%2020240617163653.png)
$L_i = -\log P(Y = y_i| X = x_i) = = -\log \frac{exp(s_{y_i})}{\sum_j exp(exp(s_j))}$：正确类别分数的负对数。
$L_i$的最小值为0(正确类别的分数为0)，最大为正无穷
当各个类别的分数几乎相等时，$L_i = - \log 1/C = \log C$
### Multiclass SVM loss(hinge loss)
"The score of the correct class should be higher than all the other scores"
![](../../img/Pasted%20image%2020240617164919.png)
**我们希望正确类别的分数比其他类别的分数至少大1**
#### 二分类Hinge loss
$L_i = max(0, 1 - y\cdot f(x))$
二分类问题中，$y$的可能取值为1或-1。我们希望$y$与$f(x)$同号，当$f(x)=y$时损失为0
